{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "with open('datasets.pickle', 'rb') as f:\n",
    "    datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_FEATURES = datasets[0].shape[0]\n",
    "TEST_RATE = 0.2\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 1024\n",
    "EARLY_BIRD = 0\n",
    "TEST_SET_NUM = int(len(datasets)*TEST_RATE)\n",
    "BEST_MODEL_PATH = 'best_model.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "d_time = datetime.datetime.now()\n",
    "folder_name = \"runs/\"+d_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "os.mkdir(folder_name)\n",
    "writer = SummaryWriter(log_dir=folder_name)\n",
    "pre_test_loss = 10000.\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(datasets, test_size=TEST_RATE, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_classes = 3\n",
    "        self.down = nn.Sequential(            \n",
    "            self._block_down(self.in_features, self.in_features*2),           \n",
    "            self._block_down(self.in_features*2, self.in_features*2),           \n",
    "            self._block_down(self.in_features*2, self.in_features),           \n",
    "            self._block_down(self.in_features, self.in_features//2),           \n",
    "            self._block_down(self.in_features//2, self.in_features//4),           \n",
    "            self._block_down(self.in_features//4, self.out_classes),                       \n",
    "            # nn.Tanh()\n",
    "        )\n",
    "        self.up = nn.Sequential(\n",
    "            self._block_down(self.out_classes,self.in_features//4),\n",
    "            self._block_down(self.in_features//4,self.in_features//2),\n",
    "            self._block_down(self.in_features//2,self.in_features),\n",
    "            self._block_down(self.in_features,self.in_features*2),\n",
    "            self._block_down(self.in_features*2,self.in_features*2),\n",
    "            self._block_down(self.in_features*2,self.in_features*2),\n",
    "            self._block_down(self.in_features*2,self.in_features),\n",
    "            nn.Tanh() # 여기만 다시 활성화            \n",
    "        )\n",
    "        self.essens=None\n",
    "    \n",
    "    def _block_down(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_channels,out_channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "    def _block_up(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_channels,out_channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        self.essens = self.down(x)\n",
    "        x2 = self.up(self.essens)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS:  0 TRAIN_LOSS 465.0870560460185 TEST_LOSS 454.86417378161127\n",
      "EPOCHS:  1 TRAIN_LOSS 455.0456781140407 TEST_LOSS 452.20233715781126\n",
      "EPOCHS:  2 TRAIN_LOSS 452.6162907260344 TEST_LOSS 449.93937879819373\n",
      "EPOCHS:  3 TRAIN_LOSS 451.2155993242125 TEST_LOSS 449.19753417984\n",
      "EPOCHS:  4 TRAIN_LOSS 450.6993026647388 TEST_LOSS 448.7433935631351\n",
      "EPOCHS:  5 TRAIN_LOSS 450.28393693066306 TEST_LOSS 448.32238037836726\n",
      "EPOCHS:  6 TRAIN_LOSS 449.90879366933257 TEST_LOSS 448.1000318532999\n",
      "EPOCHS:  7 TRAIN_LOSS 449.632133008972 TEST_LOSS 447.7931409227464\n",
      "EPOCHS:  8 TRAIN_LOSS 449.3853518997936 TEST_LOSS 447.68871206490576\n",
      "EPOCHS:  9 TRAIN_LOSS 449.2898172812981 TEST_LOSS 447.49814189083685\n",
      "EPOCHS:  10 TRAIN_LOSS 449.1672511624857 TEST_LOSS 447.40350636129136\n",
      "EPOCHS:  11 TRAIN_LOSS 449.09025385751625 TEST_LOSS 447.33010636503883\n",
      "EPOCHS:  12 TRAIN_LOSS 448.9642400981828 TEST_LOSS 447.253208751538\n",
      "EPOCHS:  13 TRAIN_LOSS 448.90433109840455 TEST_LOSS 447.1414348974761\n",
      "EPOCHS:  14 TRAIN_LOSS 448.86625715525935 TEST_LOSS 447.15597499203665\n",
      "EPOCHS:  15 TRAIN_LOSS 448.7982536951274 TEST_LOSS 447.0967153626592\n",
      "EPOCHS:  16 TRAIN_LOSS 448.7170592625672 TEST_LOSS 447.1095566145563\n",
      "EPOCHS:  17 TRAIN_LOSS 448.6351653087418 TEST_LOSS 447.00377867577714\n",
      "EPOCHS:  18 TRAIN_LOSS 448.6364644196628 TEST_LOSS 446.99718316896616\n",
      "EPOCHS:  19 TRAIN_LOSS 448.59948972422 TEST_LOSS 446.84743518478035\n",
      "EPOCHS:  20 TRAIN_LOSS 448.5670119511959 TEST_LOSS 446.78827548732426\n",
      "EPOCHS:  21 TRAIN_LOSS 448.52913787126937 TEST_LOSS 446.8411394737335\n",
      "EPOCHS:  22 TRAIN_LOSS 448.5188948813156 TEST_LOSS 446.7429063950184\n",
      "EPOCHS:  23 TRAIN_LOSS 448.502156336757 TEST_LOSS 446.8207533617723\n",
      "EPOCHS:  24 TRAIN_LOSS 448.45264022434645 TEST_LOSS 446.71607467412827\n",
      "EPOCHS:  25 TRAIN_LOSS 448.3954293780194 TEST_LOSS 446.71417596762205\n",
      "EPOCHS:  26 TRAIN_LOSS 448.39243142974027 TEST_LOSS 446.7505012210432\n",
      "EPOCHS:  27 TRAIN_LOSS 448.34526371014834 TEST_LOSS 446.5904102829947\n",
      "EPOCHS:  28 TRAIN_LOSS 448.3515094357299 TEST_LOSS 446.65086909542873\n",
      "EPOCHS:  29 TRAIN_LOSS 448.312536108101 TEST_LOSS 446.5797175674072\n",
      "EPOCHS:  30 TRAIN_LOSS 448.3035422632636 TEST_LOSS 446.59165943201197\n",
      "EPOCHS:  31 TRAIN_LOSS 448.2572739281554 TEST_LOSS 446.5502876165612\n",
      "EPOCHS:  32 TRAIN_LOSS 448.2676168497185 TEST_LOSS 446.5628290726942\n",
      "EPOCHS:  33 TRAIN_LOSS 448.2118050459217 TEST_LOSS 446.5756703245914\n",
      "EPOCHS:  34 TRAIN_LOSS 448.2228974545545 TEST_LOSS 446.550487480404\n",
      "EPOCHS:  35 TRAIN_LOSS 448.16468729213443 TEST_LOSS 446.48803002954236\n",
      "EPOCHS:  36 TRAIN_LOSS 448.1531951570644 TEST_LOSS 446.42941995765386\n",
      "EPOCHS:  37 TRAIN_LOSS 448.1609898225902 TEST_LOSS 446.5290021173076\n",
      "EPOCHS:  38 TRAIN_LOSS 448.13610685187325 TEST_LOSS 446.5253546021773\n",
      "EPOCHS:  39 TRAIN_LOSS 448.09993160930486 TEST_LOSS 446.4655453472322\n",
      "EPOCHS:  40 TRAIN_LOSS 448.1358070570453 TEST_LOSS 446.5139123971794\n",
      "EPOCHS:  41 TRAIN_LOSS 448.0895387219372 TEST_LOSS 446.45030572922195\n",
      "EPOCHS:  42 TRAIN_LOSS 448.08429231244867 TEST_LOSS 446.43181832376695\n",
      "EPOCHS:  43 TRAIN_LOSS 448.04511912160115 TEST_LOSS 446.4207758464546\n",
      "EPOCHS:  44 TRAIN_LOSS 448.06970229749015 TEST_LOSS 446.4486568525192\n",
      "EPOCHS:  45 TRAIN_LOSS 448.03577551613114 TEST_LOSS 446.39814126626237\n",
      "EPOCHS:  46 TRAIN_LOSS 448.0730000405972 TEST_LOSS 446.4696425560087\n",
      "EPOCHS:  47 TRAIN_LOSS 448.0190369715726 TEST_LOSS 446.3410801391552\n",
      "EPOCHS:  48 TRAIN_LOSS 448.0478172750524 TEST_LOSS 446.3356838154008\n",
      "EPOCHS:  49 TRAIN_LOSS 448.0515647104013 TEST_LOSS 446.2996083917831\n",
      "EPOCHS:  50 TRAIN_LOSS 448.02883026928447 TEST_LOSS 446.45425304011644\n",
      "EPOCHS:  51 TRAIN_LOSS 448.0354757213032 TEST_LOSS 446.3357837473221\n",
      "EPOCHS:  52 TRAIN_LOSS 448.03652500320095 TEST_LOSS 446.3317365045063\n",
      "EPOCHS:  53 TRAIN_LOSS 448.01633881812137 TEST_LOSS 446.3780549500653\n",
      "EPOCHS:  54 TRAIN_LOSS 448.0097433319072 TEST_LOSS 446.34287891374004\n",
      "EPOCHS:  55 TRAIN_LOSS 448.01468994656784 TEST_LOSS 446.28431880781216\n",
      "EPOCHS:  56 TRAIN_LOSS 448.0036974695443 TEST_LOSS 446.3483751694158\n",
      "EPOCHS:  57 TRAIN_LOSS 447.99080629194395 TEST_LOSS 446.35422118681646\n",
      "EPOCHS:  58 TRAIN_LOSS 447.9960027356278 TEST_LOSS 446.2975098214342\n",
      "EPOCHS:  59 TRAIN_LOSS 447.9878083436648 TEST_LOSS 446.33143670874216\n",
      "EPOCHS:  60 TRAIN_LOSS 447.957878826678 TEST_LOSS 446.2585363720965\n",
      "EPOCHS:  61 TRAIN_LOSS 448.00279808506053 TEST_LOSS 446.3114503244665\n",
      "EPOCHS:  62 TRAIN_LOSS 447.96602325283635 TEST_LOSS 446.30275624730655\n",
      "EPOCHS:  63 TRAIN_LOSS 447.9657234580084 TEST_LOSS 446.2148161564934\n",
      "EPOCHS:  64 TRAIN_LOSS 447.97221901261327 TEST_LOSS 446.31160022234855\n",
      "EPOCHS:  65 TRAIN_LOSS 447.935594077803 TEST_LOSS 446.29755978739485\n",
      "EPOCHS:  66 TRAIN_LOSS 447.968371645655 TEST_LOSS 446.23695107707874\n",
      "EPOCHS:  67 TRAIN_LOSS 447.93509441975647 TEST_LOSS 446.316346988614\n",
      "EPOCHS:  68 TRAIN_LOSS 447.95058381919876 TEST_LOSS 446.2477936905483\n",
      "EPOCHS:  69 TRAIN_LOSS 447.94963446891035 TEST_LOSS 446.1877346057998\n",
      "EPOCHS:  70 TRAIN_LOSS 447.93074739475173 TEST_LOSS 446.19083249536254\n",
      "EPOCHS:  71 TRAIN_LOSS 447.935594077803 TEST_LOSS 446.2455452223173\n",
      "EPOCHS:  72 TRAIN_LOSS 447.9024667493184 TEST_LOSS 446.1589542124428\n",
      "EPOCHS:  73 TRAIN_LOSS 447.9243517717562 TEST_LOSS 446.25169103548205\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_98132/709956177.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VENV/datavenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VENV/datavenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VENV/datavenv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VENV/datavenv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "critic = Discriminator(IN_FEATURES).to(device)\n",
    "\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "print(len(train_loader))\n",
    "critic.to(device)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = 0.\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        output = critic(data.float())\n",
    "        label = data.float()\n",
    "        loss = torch.sum(torch.square(label-output))\n",
    "        critic.zero_grad()\n",
    "        train_loss+=loss\n",
    "        loss.backward()        \n",
    "        opt_critic.step()\n",
    "    test_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = critic(data.float())\n",
    "            label = data.float()\n",
    "            loss = torch.sum(torch.square(label-output))    \n",
    "            test_loss+=loss       \n",
    "    writer.add_scalar(\"TRAIN_LOSS\",train_loss.item()/(len(datasets)-TEST_SET_NUM),epoch)\n",
    "    writer.add_scalar(\"TEST_LOSS\",test_loss.item()/TEST_SET_NUM,epoch)\n",
    "    print(\"EPOCHS: \", epoch,\"TRAIN_LOSS\",train_loss.item()/(len(datasets)-TEST_SET_NUM),\"TEST_LOSS\",test_loss.item()/TEST_SET_NUM ) \n",
    "    if pre_test_loss>test_loss and epoch>50:\n",
    "        pre_test_loss= test_loss\n",
    "        torch.save(critic.state_dict(), BEST_MODEL_PATH)\n",
    "    if pre_test_loss<test_loss and epoch>50:\n",
    "        EARLY_BIRD +=1\n",
    "        if EARLY_BIRD >100:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33b0c032e04388c0be1572b4c64e4f6e2fa9cda7ba5f9f285430d52a126a80b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('datavenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
