{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÌïôÏäµÎêú YOLO v5 Î™®Îç∏ Inference\n",
    "- inference Ìï®ÏàòÏóê weight Í≤ΩÎ°úÏôÄ Î∂ÑÏÑùÎåÄÏÉÅ img Í≤ΩÎ°ú ÏûÖÎ†•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ultralytics.com/assets/Arial.ttf to /home/hans/.config/Ultralytics/Arial.ttf...\n"
     ]
    }
   ],
   "source": [
    "from detect import run\n",
    "import cv2\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = \"./runs/train/gun_yolov5l_640_results2/weights/best.pt\"\n",
    "def inference(weight_path, img_path):\n",
    "    pred = run(weights=weight_path, source=img_path, conf_thres=0.25)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(img_path):\n",
    "    pred = inference(weight_path=weight_path, img_path=img_path)\n",
    "    x, y, w, h, conf, class_num = pred[0][0]\n",
    "    x = int(x.item())\n",
    "    y = int(y.item())\n",
    "    w = int(w.item())\n",
    "    h = int(h.item())\n",
    "    conf = conf.item()\n",
    "    img = cv2.imread(img_path)\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (255,0,0), 5)\n",
    "    img = cv2.resize(img, (640,640))\n",
    "    cv2.imshow('img', img)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v6.0-152-g26f04152 torch 1.10.0+cu113 CUDA:0 (NVIDIA GeForce GTX 1650, 3909MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46108278 parameters, 0 gradients\n",
      "image 1/1 /home/hans/yolov5/for_pre/204_102_20_0cf52d72-c02f-406e-bf27-dc9f596665fb.jpg: 288x640 Done. (0.041s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.3ms pre-process, 41.3ms inference, 0.2ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp5\u001b[0m\n",
      "YOLOv5 üöÄ v6.0-152-g26f04152 torch 1.10.0+cu113 CUDA:0 (NVIDIA GeForce GTX 1650, 3909MiB)\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 is out of bounds for dimension 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 367 layers, 46108278 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/hans/yolov5/for_pre/209_102_20_2ce92235-fa4d-4ccd-b97c-dd79b9f20342.jpg: 480x640 1 target, Done. (0.043s)\n",
      "Speed: 0.8ms pre-process, 42.9ms inference, 0.8ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp6\u001b[0m\n",
      "YOLOv5 üöÄ v6.0-152-g26f04152 torch 1.10.0+cu113 CUDA:0 (NVIDIA GeForce GTX 1650, 3909MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46108278 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/hans/yolov5/for_pre/209_102_20_1f4abb31-6231-440d-9996-ec65d3e34f3a.jpg: 320x640 1 target, Done. (0.042s)\n",
      "Speed: 0.4ms pre-process, 42.1ms inference, 1.9ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp7\u001b[0m\n",
      "YOLOv5 üöÄ v6.0-152-g26f04152 torch 1.10.0+cu113 CUDA:0 (NVIDIA GeForce GTX 1650, 3909MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46108278 parameters, 0 gradients\n",
      "YOLOv5 üöÄ v6.0-152-g26f04152 torch 1.10.0+cu113 CUDA:0 (NVIDIA GeForce GTX 1650, 3909MiB)\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n",
      "No images or videos found in /home/hans/yolov5/for_pre/204_102_20_1dc8f7f9-6482-4b92-8218-728087bd45d0.json. Supported formats are:\n",
      "images: ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']\n",
      "videos: ['mov', 'avi', 'mp4', 'mpg', 'mpeg', 'm4v', 'wmv', 'mkv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 367 layers, 46108278 parameters, 0 gradients\n",
      "image 1/1 /home/hans/yolov5/for_pre/204_102_20_0deb46d9-e3d4-48b9-a3cb-bcac3f737d1e.JPEG: 384x640 1 target, Done. (0.047s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.4ms pre-process, 47.5ms inference, 1.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp9\u001b[0m\n",
      "YOLOv5 üöÄ v6.0-152-g26f04152 torch 1.10.0+cu113 CUDA:0 (NVIDIA GeForce GTX 1650, 3909MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46108278 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/hans/yolov5/for_pre/102_103_20_f03b1689-9132-4286-865c-06b9b6f7fd48.jpg: 320x640 1 target, Done. (0.038s)\n",
      "Speed: 0.3ms pre-process, 37.7ms inference, 0.8ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp10\u001b[0m\n",
      "YOLOv5 üöÄ v6.0-152-g26f04152 torch 1.10.0+cu113 CUDA:0 (NVIDIA GeForce GTX 1650, 3909MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46108278 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/hans/yolov5/for_pre/204_102_20_1f3e18d3-2892-4164-bdbc-f429ed3b8b84.jpg: 288x640 1 target, Done. (0.043s)\n",
      "Speed: 0.3ms pre-process, 43.2ms inference, 0.8ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp11\u001b[0m\n",
      "YOLOv5 üöÄ v6.0-152-g26f04152 torch 1.10.0+cu113 CUDA:0 (NVIDIA GeForce GTX 1650, 3909MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46108278 parameters, 0 gradients\n",
      "image 1/1 /home/hans/yolov5/for_pre/209_102_20_0fbe04f6-6274-4127-be66-9e033b63daec.jpg: 320x640 1 target, Done. (0.037s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.3ms pre-process, 36.8ms inference, 0.8ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp12\u001b[0m\n",
      "YOLOv5 üöÄ v6.0-152-g26f04152 torch 1.10.0+cu113 CUDA:0 (NVIDIA GeForce GTX 1650, 3909MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46108278 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/hans/yolov5/for_pre/209_203_20_0f27299b-71ab-4592-b819-0d7e307d4ebe.JPG: 384x640 1 target, Done. (0.040s)\n",
      "Speed: 0.5ms pre-process, 39.7ms inference, 0.8ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp13\u001b[0m\n",
      "YOLOv5 üöÄ v6.0-152-g26f04152 torch 1.10.0+cu113 CUDA:0 (NVIDIA GeForce GTX 1650, 3909MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46108278 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/hans/yolov5/for_pre/209_203_20_1dac7ef3-c2f6-456c-abe7-7c3951a023fb.jpg: 320x640 Done. (0.036s)\n",
      "Speed: 0.4ms pre-process, 35.9ms inference, 0.2ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp14\u001b[0m\n",
      "YOLOv5 üöÄ v6.0-152-g26f04152 torch 1.10.0+cu113 CUDA:0 (NVIDIA GeForce GTX 1650, 3909MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 is out of bounds for dimension 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46108278 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/hans/yolov5/for_pre/209_102_20_3ad92ea3-add1-4f48-a7e1-081577384666.jpg: 384x640 1 target, Done. (0.040s)\n",
      "Speed: 0.5ms pre-process, 40.0ms inference, 0.9ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp15\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "file_list = os.listdir(\"./for_pre\")\n",
    "for idx, name in enumerate(file_list):\n",
    "    try:\n",
    "        output('./for_pre/'+name)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    if idx == 10:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 640, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8802181460>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59b807c8e71eea35cddd73797a319c46f6da58d130705c0b1685d9f316ed1341"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('torch': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
